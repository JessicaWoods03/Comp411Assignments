{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM86+HmyzXP8KRP7KP62KYS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JessicaWoods03/Comp411Assignments/blob/main/Homework7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SExl5qknKlZ4"
      },
      "source": [
        "Jessica Woods\n",
        "\n",
        "Homework 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjgJbTwEe5AA"
      },
      "source": [
        "1. Why the holdout method for model selection suggests to seperate the data into three parts:\n",
        "a training set\n",
        "a validation\n",
        "and a test set?\n",
        "\n",
        "It is hyperparameters, this hold out validation model is a sensitive model to data. \n",
        "\n",
        "2. Given a data set (wine), split data (20% test), apply pipeline to standardize the data, classify the data using KNeighborsClassifier(n_neighbors = 10) print accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1PD65FDMpyi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp5yoLKhTibq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27771d61-411d-45a3-e8f8-7ae6b378ed44"
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = datasets.load_wine()\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "#Data split into %20\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
        "##used the KNeighborsClassifer and the Website scikit learn, that our teacher suggested for further help :)\n",
        "pipe_lr = make_pipeline(StandardScaler(), PCA(n_components = 2), KNeighborsClassifier(n_neighbors=10))\n",
        "\n",
        "pipe_lr.fit(X_train1, y_train1)\n",
        "\n",
        "y_pred = pipe_lr.predict(X_test1)\n",
        "print('Test Accuracy: %.3f' % pipe_lr.score(X_test1, y_test1))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCa484DHhn0C"
      },
      "source": [
        "3. What is a learning curve? Base on learning curve, how do you know if the model is over fitting or not?\n",
        "\n",
        "Depends on who your asking LOL- My grandpa would say a learning curve is when you mess up good LOL- Learning curves are based on bias, variances,  and tradeoffs. Large gaps indicate overfitting.\n",
        "\n",
        "4. In the above data set, fit KNN using 10-fold cross validation and grid search to optimize the number of neighbors, print the optimized parameters and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4wNDo7p7Hju",
        "outputId": "268f51e0-c6f2-408a-f156-47f18c72ea97"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators=300, random_state=0)\n",
        "\n",
        "scores = cross_val_score(estimator=classifier, X=X_train1,\n",
        "                         y=y_train1,\n",
        "                         cv=10, ##10fold \n",
        "                         )\n",
        "print(scores)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.         1.         1.         1.         1.         0.92857143\n",
            " 1.         0.92857143 0.92857143 1.        ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHuH7tihDErd",
        "outputId": "a0040cfd-1205-4830-a0d0-b86bf906da93"
      },
      "source": [
        "##This is one will run for a few minutes before it works- but it works\n",
        "grid_param = {\n",
        "    'n_estimators': [100, 300, 500, 800, 1000],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "gd_sr = GridSearchCV(estimator=classifier,\n",
        "                     param_grid=grid_param,\n",
        "                     scoring='accuracy',\n",
        "                     cv=10,\n",
        "                     n_jobs=-1)\n",
        "\n",
        "\n",
        "gd_sr.fit(X_train1, y_train1)\n",
        "best_parameters = gd_sr.best_params_\n",
        "print(best_parameters)\n",
        "\n",
        "best_result = gd_sr.best_score_\n",
        "print(best_result)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'bootstrap': False, 'criterion': 'gini', 'n_estimators': 100}\n",
            "0.9857142857142858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9qo_vbFhx6"
      },
      "source": [
        "5. Calculate the accuracy precision and recall based on th following confusion matrix-\n",
        "n= 165\n",
        "predicted yes 110 times\n",
        "predicted no 55 times\n",
        "\n",
        "Accuracy = (100 + 50) / 165 = .91\n",
        "\n",
        "misclassifciation rate = (10 + 5)/165 = .09\n",
        "\n",
        "True Positive Rate = 100/105 = .95\n",
        "\n",
        "True Nagative Rate = 50/60 = .83\n",
        "\n",
        "False Positive Rate = 10/60 =.17\n",
        "\n",
        "50 out of 55 for the accuracy of the no call\n",
        "\n",
        "50 out of 60 for the recall of the no call\n",
        "\n",
        "6. \"For imbalanced dataset, accuracy is not a valid measurement of model performance\", discuss why is thaqt and what metrics you would like to use?\n",
        "\n",
        "Found a neat website that discusses this in depth - \n",
        "https://towardsdatascience.com/metrics-for-imbalanced-classification-41c71549bbb5#:~:text=%20Metrics%20for%20Imbalanced%20Classification%20%201%20Problem,Characteristic%20and%20initially%20was%20designed%20as...%20More%20\n",
        "\n",
        "This guy suggest Matthews Correlation Coefficient and F1-score, with having a calculas background I can look at the equations and agree to that as well. Even if our notes only go over true and false ratio's, these equations are much better suited for the imbalances.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CSmdyfEFrGF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}